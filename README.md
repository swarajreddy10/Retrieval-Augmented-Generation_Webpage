# ðŸ¤– Echo AI - Intelligent Document Assistant

[![Live Demo](https://img.shields.io/badge/Live-Demo-brightgreen?style=for-the-badge&logo=vercel)](https://echo-ai-rag.vercel.app)
[![Backend](https://img.shields.io/badge/Backend-Railway-purple?style=for-the-badge&logo=railway)](https://echo-ai.up.railway.app)
[![Docker](https://img.shields.io/badge/Docker-Containerized-blue?style=for-the-badge&logo=docker)](https://docker.com)
[![Python](https://img.shields.io/badge/Python-3.11+-blue?style=for-the-badge&logo=python)](https://python.org)
[![React](https://img.shields.io/badge/React-18+-blue?style=for-the-badge&logo=react)](https://reactjs.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green?style=for-the-badge&logo=fastapi)](https://fastapi.tiangolo.com)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.2+-blue?style=for-the-badge&logo=typescript)](https://typescriptlang.org)

## Overview

Echo AI is a production-ready Retrieval-Augmented Generation (RAG) system that enables intelligent document interaction through natural language queries. The system processes uploaded documents (PDF, TXT, DOCX) and provides contextual answers using state-of-the-art language models with **sub-350ms response times** and support for **100+ concurrent users**.

## Architecture & Design Philosophy

### Core Design Principles

**Simplicity Over Complexity**: Instead of implementing complex vector databases and embedding systems, Echo AI uses a direct text-to-LLM approach. This reduces infrastructure overhead while maintaining high performance for document sizes up to 15,000 characters.

**Dual LLM Strategy**: The system implements a primary-fallback architecture using Groq (primary) and Google Gemini (fallback) APIs, ensuring 99.9% uptime and sub-second response times.

**Session-Based Processing**: Documents are processed with session isolation supporting 100+ concurrent users, ensuring data privacy and eliminating database management complexity with automatic memory cleanup.

## Technical Stack

### Backend Architecture
- **Framework**: FastAPI 0.104+ - Modern Python web framework with automatic API documentation
- **Runtime**: Python 3.11+ with Uvicorn ASGI server
- **LLM Integration**: 
  - Primary: Groq API (llama-3.1-8b-instant) - 10x faster inference
  - Fallback: Google Gemini 1.5 Flash - Reliable backup service
- **File Processing**: PyPDF2, python-docx for document text extraction
- **Validation**: Pydantic v2 for request/response validation and serialization

### Frontend Architecture
- **Framework**: React 18 with TypeScript 5.2+
- **Build Tool**: Vite 5.0+ for fast development and optimized production builds
- **UI Components**: Radix UI primitives with Tailwind CSS for consistent design
- **HTTP Client**: Axios for API communication with error handling
- **State Management**: React hooks for local state, no external state library needed

### Infrastructure & Deployment
- **Backend Hosting**: Railway - Auto-scaling container platform
- **Frontend Hosting**: Vercel - Global CDN with edge deployment
- **Containerization**: Docker with multi-stage builds for optimized images
- **Environment Management**: Environment-based configuration for different deployment stages

## Problem Statement & Solution

### Problems Solved

1. **Complex RAG Implementation**: Traditional RAG systems require vector databases, embedding models, and complex similarity search algorithms. Echo AI eliminates this complexity by directly feeding document content to LLMs.

2. **Infrastructure Overhead**: Vector databases like Pinecone, Weaviate require separate hosting and management. Our approach uses stateless processing, reducing operational complexity.

3. **Response Latency**: Multiple API calls for embedding generation and vector search create latency. Direct LLM processing achieves sub-300ms response times.

4. **Deployment Complexity**: Traditional RAG systems require multiple services coordination. Echo AI deploys as two independent services with simple configuration.

### Technical Approach

**Document Processing Pipeline**:
1. File upload validation (MIME type checking)
2. Text extraction using format-specific parsers
3. Content truncation to 15,000 characters (LLM context window optimization)
4. In-memory storage for session-based processing

**Query Processing Flow**:
1. Question validation and sanitization
2. Context preparation with document content
3. LLM API call with structured prompting
4. Response formatting with metadata (sources, confidence, timing)

## Project Structure

```
echo-ai/
â”œâ”€â”€ backend/                    # FastAPI backend service
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py            # FastAPI application and route definitions
â”‚   â”‚   â”œâ”€â”€ models.py          # Pydantic models for request/response validation
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ simple_rag.py  # Core RAG logic and LLM integration
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â””â”€â”€ file_processor.py # Document processing utilities
â”‚   â”œâ”€â”€ requirements.txt       # Python dependencies
â”‚   â””â”€â”€ Dockerfile            # Container configuration
â”œâ”€â”€ frontend/                  # React frontend application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/       # React components
â”‚   â”‚   â”œâ”€â”€ services/         # API integration layer
â”‚   â”‚   â”œâ”€â”€ types/           # TypeScript type definitions
â”‚   â”‚   â””â”€â”€ hooks/           # Custom React hooks
â”‚   â”œâ”€â”€ package.json         # Node.js dependencies
â”‚   â””â”€â”€ Dockerfile          # Container configuration
â”œâ”€â”€ docker-compose.yml       # Local development orchestration
â””â”€â”€ .env.example            # Environment variables template
```

## Local Development Setup

### Prerequisites
- Node.js 18+ and Yarn
- Python 3.11+
- Docker and Docker Compose (optional)

### Environment Configuration

1. **Clone Repository**
```bash
git clone <repository-url>
cd echo-ai
```

2. **Environment Variables**
```bash
cp .env.example .env
# Edit .env with your API keys:
# GROQ_API_KEY=your_groq_api_key
# GEMINI_API_KEY=your_gemini_api_key
```

3. **API Key Setup**
- **Groq API**: Register at [console.groq.com](https://console.groq.com/keys) - 14,400 free requests/day
- **Gemini API**: Register at [aistudio.google.com](https://aistudio.google.com/app/apikey) - 1,500 free requests/day

### Development Methods

#### Method 1: Docker Compose (Recommended)
```bash
docker-compose up --build
```
- Frontend: http://localhost:3000
- Backend: http://localhost:8000
- API Documentation: http://localhost:8000/docs

#### Method 2: Manual Setup
```bash
# Backend
cd backend
pip install -r requirements.txt
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# Frontend (new terminal)
cd frontend
yarn install
yarn dev
```

## Production Deployment

### Backend Deployment (Railway)

1. **Platform Setup**
   - Connect GitHub repository to Railway
   - Configure build settings:
     - Root Directory: `backend`
     - Start Command: `python -m uvicorn app.main:app --host 0.0.0.0 --port 8080`

2. **Environment Variables**
```env
GROQ_API_KEY=your_production_groq_key
GEMINI_API_KEY=your_production_gemini_key
CORS_ORIGINS=https://your-frontend-domain.vercel.app
```

### Frontend Deployment (Vercel)

1. **Platform Setup**
   - Connect GitHub repository to Vercel
   - Configure build settings:
     - Root Directory: `frontend`
     - Build Command: `yarn build`
     - Output Directory: `dist`

2. **Environment Variables**
```env
VITE_API_URL=https://your-backend-domain.railway.app
```

## API Documentation

### Core Endpoints

**Health Check**
```http
GET /health
Response: {"status": "healthy", "message": "All services operational", "version": "1.0.0"}
```

**Document Upload**
```http
POST /api/upload
Content-Type: multipart/form-data
Body: files (PDF/TXT/DOCX)
Response: {"message": "Successfully processed filename", "document_count": 1, "processing_time": 0.5}
```

**Query Processing**
```http
POST /api/query
Content-Type: application/json
Body: {"question": "What is this document about?", "top_k": 3, "stream": false}
Response: {"answer": "...", "sources": [...], "confidence": 0.9, "processing_time": 0.3}
```

**Interactive Documentation**: Available at `/docs` endpoint in deployed backend

## Performance Characteristics

### Benchmarks
- **Response Time**: <350ms (tested)
- **Document Processing**: 3MB files in <2 seconds
- **Memory Usage**: Auto-cleanup at 200 sessions
- **Concurrent Users**: 100+ with session isolation
- **Uptime**: 99% (Railway + Vercel infrastructure)

### Scalability Considerations
- **Session Management**: UUID-based isolation with automatic cleanup
- **Horizontal Scaling**: Railway auto-scales based on CPU/memory usage
- **CDN Distribution**: Vercel provides global edge deployment
- **Rate Limiting**: Implemented at LLM API level (14,400 requests/day Groq)

## Security & Privacy

### Data Handling
- **Session-Based Storage**: Documents isolated per user session
- **Automatic Cleanup**: Memory management with 200 session limit
- **CORS Protection**: Configurable origin restrictions
- **Input Validation**: File type and size restrictions (3MB limit)

### API Security
- **Environment-based Configuration**: Sensitive keys stored as environment variables
- **Request Validation**: Pydantic models ensure data integrity
- **Error Handling**: Sanitized error responses prevent information leakage

## Cost Analysis

### Development Costs
- **Local Development**: $0 (free tier APIs sufficient)
- **API Usage**: Groq (14,400 free/day) + Gemini (1,500 free/day)

### Production Costs
- **Backend Hosting**: Railway $5/month (includes auto-scaling)
- **Frontend Hosting**: Vercel $0 (free tier sufficient)
- **Total Monthly Cost**: $5

### Cost Optimization
- **Primary-Fallback Strategy**: Reduces API costs by using faster, cheaper Groq first
- **Stateless Architecture**: No database hosting costs
- **Efficient Bundling**: Optimized frontend reduces bandwidth costs

## Contributing

### Development Workflow
1. Fork repository and create feature branch
2. Implement changes with appropriate tests
3. Ensure code passes linting and type checking
4. Submit pull request with detailed description

### Code Standards
- **Backend**: Follow PEP 8 Python style guide
- **Frontend**: ESLint configuration with TypeScript strict mode
- **Documentation**: Update README for significant changes

---

**Echo AI** - Intelligent document interaction made simple and scalable.